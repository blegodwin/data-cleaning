# Data Quality Validation and Data Cleaning Process


## Overview

From messy to clean: This repository documents a project focused on ensuring the accuracy and reliability of a dataset through comprehensive data quality validation and cleaning. The project aimed to ensure that the data is accurate, consistent, and reliable. 

The project identify and rectify inconsistencies, errors, and missing values to enhance data integrity and usability for analysis through a process that is documented.

Python programming language and its manipulation and computational libraries, pandas and numpy respectively are employed.

The original .csv file and supplementary word document is stored in this repository.

## Project Highlights

- Data Quality Standards and Requirements: This is define as the accepted decimal place for grade columns, the data types, text casing and value data types and consistency for computation.

- Data Profiling:thorough examination of the dataset to understand its structure, and anomalies. Potential data quality issues, such as missing values, duplicates, outliers, and inconsistencies were identified.

- Data Cleaning and Transformation: Python pandas and numpy libraries were employed as data cleaning tools to address identified issues such as and not limited to; removing duplicates, handling missing values,truncating different data types recorded as single value, standardising formats and data types to ensure uniformity.

- Data Enrichment: The dataset is augumented with additional relevant information from external sources (The word doc "codebook_food") to enhance understanding. 

- Implement Data Validation Rules:Data validation rules were applied to ensure that data adheres to predefined standards.
These rules included maintaining the data value types, encoding missing values, truncating values and maintaining decimal places.

- Documentation:The data quality validation and cleaning processes and  rules were documented  for transparency and reproducibility.